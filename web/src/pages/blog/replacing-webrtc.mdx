---
layout: "@/layouts/global.astro"
title: Replacing WebRTC
author: kixelated
---

# Replacing WebRTC

The long and winding path to using _something_ else.

## tl;dr

If you primarily use WebRTC for...

-   **real-time media**: it will take a while to make something better; we're working on it.
-   **data channels**: WebTransport is amazing and _actually_ works.
-   **peer-to-peer**: you're stuck with WebRTC for the forseeable future.

## Disclaimer

I spent over a year building/optimizing a full WebRTC stack @ Twitch using [pion](https://github.com/pion/webrtc).
We ultimately scrapped it and my use-case was quite custom so your millage will vary.
Also, some of these issues may have been addressed since then.

## Why WebRTC?

Google released WebRTC in 2011 as a way of fixing a very specific problem:

> How do we build Google Meet?

Back then, the web was a very different place.
Flash was the only way to do live media and it was a _mess_.
HTML5 video was primarily for pre-recorded content.
It personally took me until 2015 to write a [HTML5 player for Twitch](https://reddit.com/r/Twitch/comments/3hqfkw/the_csgo_client_embeds_the_twitch_html5_player/) using [MSE](https://developer.mozilla.org/en-US/docs/Web/API/Media_Source_Extensions_API), and we're still talking 5+ seconds of latency on a good day.

Transmitting video over the internet _in real-time_ is hard.

You need a tight coupling between the video encoding and the network to avoid any form of queuing, which adds latency.
This effectively rules out TCP and forces you to use UDP.
But now you also need a video encoder/decoder that can deal with packet loss without spewing artifacts everywhere.

<figure>
	![Video artifacts](/blog/replacing-webrtc/artifact.png)
	<figcaption>
		[Source](https://flashphoner.com/10-important-webrtc-streaming-metrics-and-configuring-prometheus-grafana-monitoring/).
		Example of Artifacts caused by packet loss.
	</figcaption>
</figure>

Google (correctly) determined that it would be impossible to solve these problems piecewise with new web standards.
The approach instead was to create [libwebrtc](https://webrtc.googlesource.com/src/), the defacto WebRTC implementation that still ships with all browsers.
It does everything, from networking to video encoding/decoding to data transfer, and it does it remarkably well.
It's actually quite a feat of software engineering, _especially_ the part where Google managed to convince Apple/Mozilla to embed a full media/networking stack into their browsers.

My favorite part about WebRTC is that it manages to leverage existing standards.
WebRTC is not really a protocol, but rather a collection of protocols: [ICE](https://datatracker.ietf.org/doc/html/rfc8445), [STUN](https://datatracker.ietf.org/doc/html/rfc5389), [TURN](https://datatracker.ietf.org/doc/html/rfc5766), [DTLS](https://datatracker.ietf.org/doc/html/rfc6347), [RTP/RTCP](https://datatracker.ietf.org/doc/html/rfc3550), [SRTP](https://datatracker.ietf.org/doc/html/rfc3711), [SCTP](https://datatracker.ietf.org/doc/html/rfc4960), [SDP](https://datatracker.ietf.org/doc/html/rfc4566), [mDNS](https://datatracker.ietf.org/doc/html/rfc6762), etc.
Throw a [Javascript API](https://www.w3.org/TR/webtransport/) on top of these and you have WebRTC.

<figure>
	![WebRTC protocols and layers](/blog/replacing-webrtc/layers.png)
	<figcaption>[Source](https://hpbn.co/webrtc/). That's a lot of protocols layered on top of each other.</figcaption>
</figure>

## Why not WebRTC?

I wouldn't be writing this blog post if WebRTC was perfect.
The core issue is that WebRTC is not a protocol; it's a monolith.

WebRTC does a lot of things, let's break down it down piece by piece:

-   [Media](#media): a full capture/encoding/networking/rendering pipeline.
-   [Data](#data): reliable/unreliable messages.
-   [P2P](#p2p): peer-to-peer connectability.
-   [SFU](#sfu): a relay that selectively forwards media.

### Media

The WebRTC media stack is designed for conferencing and does an amazing job at it.
The problems start when you try to use it for anything else.

My last project at Twitch was to reduce latency by replacing HLS with WebRTC for delivery.
This seems like a no-brainer at first, but it quickly turned into [death by a thousand cuts](https://docs.google.com/document/d/1OTnJunbpSJchdj8XI3GU9Fo-RUUFBqLO1AhlaKk5Alo/edit?usp=sharing).
The biggest issue was that the user experience was just terrible.
Twitch doesn't need the same aggressive latency as Google Meet, but WebRTC is hard-wired to compromise on quality.

In general, it's quite difficult to customize WebRTC outside of a few configurable modes.
It's a black box that you turn on, and if it works it works.
And if it doesn't work, then you have to deal with the pain that is [forking libwebrtc](https://github.com/webrtc-sdk/libwebrtc)... or just hope Google fixes it for you.

The protocol has some wiggle room and I really enjoyed my time tinkering with [pion](https://github.com/pion/webrtc).
But you're ultimately bound by the browser implementation, unless you don't need web support, in which case you don't need WebRTC.

### Data

WebRTC also has a data channel API, which is particularly useful because [until recently](https://developer.mozilla.org/en-US/docs/Web/API/WebTransport), it's been the only way to send/receive unreliable messages from a browser.

In fact, many companies use WebRTC data channels to avoid the WebRTC media stack (ex. Zoom).
I went down this path too, attempting to send each frame as an unreliable message, but ultimately it doesn't work due to fundamental flaws with [SCTP](https://www.rfc-editor.org/rfc/rfc4960.html).

I eventually hacked "datagram" support into SCTP by breaking frames into unreliable messages below the MTU size.
Finally! UDP in the browser, but at what cost:

-   a convoluted handshake that takes at least 10 (!) round trips.
-   2x the packets, because libsctp immediately ACKs every "datagram".
-   a custom SCTP implementation, which means the browser can't send "datagrams".

Oof. Fortunately there's now a [better way](https://developer.mozilla.org/en-US/docs/Web/API/WebTransport/datagrams) to send datagrams.

### P2P

The best and worst part about WebRTC is that it supports peer-to-peer.
The ICE handshake is extremely complicated:

-   You need both peers to generate a SDP offer/answer.
-   You need a public server (usually HTTPS/WebSocket) to exchange the offer/answer.
-   Some networks use symmetric NATs, so you need a fallback TURN server.
-   Some networks block UDP, so you need a fallback TCP TURN server.
-   Some networks only support IPv4 or IPv6 internally, so you should support dual-stack.
-   Some clients only support mDNS, so you have to figure that out too.

Most conferencing solutions these days are actually client-server for better QoS.
However, you're still forced to perform the complicated ICE handshake designed for peer-to-peer.
This has major architecture ramifications, but I'll save that for another blog post.

### SFU

Last but not least, WebRTC is scaled out using SFUs (Selective Forwarding Units).

<figure>
	![SFU example](/blog/replacing-webrtc/sfu.png)
	<figcaption>
		[Source](https://blog.livekit.io/scaling-webrtc-with-distributed-mesh/). Participants send to a central server,
		rather than directly to each other.
	</figcaption>
</figure>

The problem with SFUs is subtle: they're custom.
It requires a lot of business logic to determine _which_ packets to selectively forward.

-   Detecting congestion: you need something like [GCC](https://datatracker.ietf.org/doc/html/draft-ietf-rmcat-gcc-02), [NADA](https://datatracker.ietf.org/doc/html/rfc8698), [SCReAM](https://github.com/EricssonResearch/scream), etc. And the peer needs to support it too.
-   Dropping packets: the SFU has to partially parse the media contents to decide which packets to drop, otherwise you'll cause artiracts.
-   Routing packets: A single server doesn't scale, so now you need to figure out how to route packets between servers/clients.

What we ran into at Twitch is that push-based SFUs share very little in common with traditional pull-based CDNs.
One team is optimizing WebRTC, another team is optimizing HTTP, and they're not talking to each other.

This is the fundamental reason why HLS/DASH uses HTTP: economies of scale.

# Replacing WebRTC

Okay enough ranting about what's wrong, let's fix it.

First off, **WebRTC is not going anywhere**. It does a fantastic job at what it was designed for: conferencing.
It will take a long time before it's possible to reach feature/performance parity with WebRTC.

## **Web**Support

Before you can replace **Web**RTC, you need some new technologies that also start with **Web**.
Fortunately, we now have **Web**Codecs and **Web**Transport.

### WebCodecs

[WebCodecs](https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API) is a new API for encoding/decoding media in the browser.
It's remarkably simple:

1. Capture input via [canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API) or a [media device](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia).
2. [VideoEncoder](https://developer.mozilla.org/en-US/docs/Web/API/VideoEncoder): Input raw frames, output encoded frames.
3. Transfer those frames somehow. (ex. [WebTransport](#webtransport))
4. [VideoDecoder](https://developer.mozilla.org/en-US/docs/Web/API/VideoDecoder): Input encoded frames, output raw frames.
5. Render output via [canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API) or just marvel at the pixel data.

The catch is that the application is responsible for all timing.
That means you need to choose when to render each frame via [requestAnimationFrame](https://developer.mozilla.org/en-US/docs/Web/API/window/requestAnimationFrame).
In fact, you need to choose when to render each audio _sample_ via [AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet).

The upside is that now your web application gets full control and can implement WebRTC-like functionality.
For example, temporarily playing audio with frozen video before suddenly jumping forward.

[caniuse](https://caniuse.com/webcodecs)

### WebTransport

[WebTransport](https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API) is a new API for transmitting data over the network.
Think of it like WebSockets, but with a few key differences:

-   [QUIC](https://www.rfc-editor.org/rfc/rfc9000.html) not TCP.
-   Provides independent streams that can be closed/prioritized.
-   Provides datagrams that can be dropped.

QUIC has too many benefits to enumerate, but some highlights:

-   Fully encrypted
-   Congestion controlled (even datagrams)
-   1-RTT handshake
-   Multiplexed over a single UDP port
-   Transparent network migration (ex. switching from Wifi to LTE)
-   Used for HTTP/3

That last one is surprisingly important: WebTransport will share all of the optimizations that HTTP/3 receives.
A HTTP/3 server can simultaneously serve WebTransport sessions and HTTP requests over the same connection.

[caniuse](https://caniuse.com/webtransport)

## But how?

Okay, so we have WebCodecs and WebTransport, but are they actually useful?

I alluded to the secret behind reducing latency earlier: avoiding queues.
Queuing can occur at any point in the media pipeline.

| Capture | Encode | Send | Receive | Decode | Render |
| :-----: | :----: | :--: | :-----: | :----: | :----: |
|   -->   |  -->   | -->  |   -->   |  -->   |  -->   |

Let's start with the easy one.
[WebCodecs](#webcodecs) allows you to avoid queuing almost entirely.
You'll still want some form of jitter buffer before rendering, but that's it.

|    Capture    |    Encode     | Send | Receive |    Decode     |    Render     |
| :-----------: | :-----------: | :--: | :-----: | :-----------: | :-----------: |
| **WebCodecs** | **WebCodecs** |  ?   |    ?    | **WebCodecs** | **WebCodecs** |

The tricky part is the bit in the middle, the network.

### The Internet of Queues

The internet is a [series of tubes](https://en.wikipedia.org/wiki/Series_of_tubes).
You put packets in one end and they eventually come out of the other end, kinda.
This section will get an entire blog post in the future, but until then, let's over-simplify things.

Every packet you send fights with other packets on the internet.

-   If routers have sufficient throughput, then the only limit is the speed of light.
-   If routers don't have sufficient throughput, then packets will be queued.
-   If those queues are full, then packets will be dropped instead.

### Detecting Queuing

The entire point of congestion control is to detect this situation and back off.
Here's a quick summary of TCP congestion control algorithms:

-   Loss-based algorithms ([Reno/CUBIC](https://en.wikipedia.org/wiki/TCP_congestion_control)) use packet loss as a signal that a queue is full.
-   Delay-based algorithms ([BBR](https://research.google/pubs/pub45646/)/[COPA](https://web.mit.edu/copa/)) use RTT as a signal that packets are being queued.
-   ECN-based algorithms ([L4S](https://www.rfc-editor.org/rfc/rfc9331.html)) use an explicit signal from routers that packets are being queued.

To keep latency low, you need to detect queuing as early as possible.
WebRTC accomplishes that with [transport-wide-cc](https://webrtc.googlesource.com/src/+/refs/heads/main/docs/native-code/rtp-hdrext/transport-wide-cc-02/README.md), something that is probably necessary for QUIC too until widespread L4S support.

### Reducing Bitrate

Once you detect queuing, the application needs to send fewer bytes if it wants them to arrive in a timely manner.

For real-time media, we can lower the bitrate by either:

1.  Reducing the encoder bitrate
2.  Dropping encoded media

We can easily drop the encoder bitrate with [WebCodecs](#webcodecs).
However, this only applies to future frames; we can't retroactively re-encode frames already queued.
It's also not an option for CDNs since it's too expensive to retranscode for each viewer.

So we have to drop encoded media.
This is the fundamental problem with TCP: once you queue up data in the TCP socket, you can't drop it.

However there are actually quite a few ways of dropping media with [WebTransport](#webtransport):

1. Use datagrams and choose which packets to transmit. (like WebRTC)
2. Use QUIC streams and reset them to stop transmitting. (like [RUSH](https://www.ietf.org/archive/id/draft-kpugin-rush-00.html))
3. Use QUIC streams and prioritize them to prefer newer media. (like [Warp](https://www.youtube.com/watch?v=PncdrMPVaNc))

I'm biased because I made the 3rd one.
I use WebTransport's [sendOrder](https://www.w3.org/TR/webtransport/#dom-webtransportsendstreamoptions-sendorder) to instruct the QUIC stack which streams to prioritize.
But that deserves an entire blog post on its own.

# Replacing WebRTC

But to actually replace WebRTC, we need a standard. Anybody can make their own UDP-based protocol (_and they do_), using this new web tech (_and they will_).

What sets [Media over QUIC](https://datatracker.ietf.org/wg/moq/about/) apart is that we're doing it through the IETF, the same organization that standardized WebRTC... and QUIC... and HTTP... and almost every other protocol that I've linked.

It's going to take a while. A few years at least.<br />
It's going to take a lot of idiots like myself who want to replace WebRTC. <br />
It's going to take a lot of companies who are willing to bet on a new standard.<br />

And there are major flaws with both **WebCodecs** and **WebTransport** that still need to be addressed before we'll ever reach WebRTC parity.
To name a few:

-   We need something like [transport-wide-cc](https://webrtc.googlesource.com/src/+/refs/heads/main/docs/native-code/rtp-hdrext/transport-wide-cc-02/README.md) in QUIC.
-   We need better [congestion control](https://www.w3.org/TR/webtransport/#dom-webtransportoptions-congestioncontrol) in browsers.
-   We need echo cancellation in WebCodecs, which might be possible already?
-   We need [FEC](https://datatracker.ietf.org/doc/draft-michel-quic-fec/) in QUIC, at least to experiment.
-   We need more encoding options, like non-reference frames or SVC.
-   Oh yeah and full browser support: [WebCodecs](https://caniuse.com/webcodecs) - [WebTransport](https://caniuse.com/webtransport)

Hit me up on [Discord](https://discord.gg/FCYF3p99mr) (@kixelated) if you want to help or just chat.
