---
layout: "@/layouts/global.astro"
title: Replacing WebRTC
author: kixelated
---

# Replacing WebRTC

The long and winding path, searching for _something else_.
Written by [@kixelated](https://github.com/kixelated).

## tl;dr

If you primarily use WebRTC for...

-   **real-time media**: it will take a while to make something comparable; we're working on it.
-   **data channels**: WebTransport is amazing and _actually_ works.
-   **peer-to-peer**: you're stuck with WebRTC for the forseeable future.

## Disclaimer

I spent over a year building/optimizing a full WebRTC stack @ Twitch using [pion](https://github.com/pion/webrtc).
We ultimately scrapped it and my use-case was quite custom so your millage will vary.
Also, some of these issues may have been addressed since then.

## Why WebRTC?

Google released WebRTC in 2011 as a way of fixing a very specific problem:

> How do we build Google Meet?

Back then, the web was a very different place.
Flash was the only way to do live media and it was a _mess_.
HTML5 video was primarily for pre-recorded content.
It personally took me until 2015 to write a [HTML5 player for Twitch](https://reddit.com/r/Twitch/comments/3hqfkw/the_csgo_client_embeds_the_twitch_html5_player/) using [MSE](https://developer.mozilla.org/en-US/docs/Web/API/Media_Source_Extensions_API), and we're still talking 5+ seconds of latency on a good day.

Transmitting video over the internet _in real-time_ is hard.

You need a tight coupling between the video encoding and the network to avoid any form of queuing, which adds latency.
This effectively rules out TCP and forces you to use UDP.
But now you also need a video encoder/decoder that can deal with packet loss without spewing artifacts everywhere.

<figure>
	![Video artifacts](/blog/replacing-webrtc/artifact.png)
	<figcaption>
		[Source](https://flashphoner.com/10-important-webrtc-streaming-metrics-and-configuring-prometheus-grafana-monitoring/).
		Example of Artifacts caused by packet loss.
	</figcaption>
</figure>

Google (correctly) determined that it would be impossible to solve these problems piecewise with new web standards.
The approach instead was to create [libwebrtc](https://webrtc.googlesource.com/src/), the defacto WebRTC implementation that still ships with all browsers.
It does everything, from networking to video encoding/decoding to data transfer, and it does it remarkably well.
It's actually quite a feat of software engineering, _especially_ the part where Google managed to convince Apple/Mozilla to embed a full media/networking stack into their browsers.

My favorite part about WebRTC is that it manages to leverage existing standards.
WebRTC is not really a protocol, but rather a collection of protocols: [ICE](https://datatracker.ietf.org/doc/html/rfc8445), [STUN](https://datatracker.ietf.org/doc/html/rfc5389), [TURN](https://datatracker.ietf.org/doc/html/rfc5766), [DTLS](https://datatracker.ietf.org/doc/html/rfc6347), [RTP/RTCP](https://datatracker.ietf.org/doc/html/rfc3550), [SRTP](https://datatracker.ietf.org/doc/html/rfc3711), [SCTP](https://datatracker.ietf.org/doc/html/rfc4960), [SDP](https://datatracker.ietf.org/doc/html/rfc4566), [mDNS](https://datatracker.ietf.org/doc/html/rfc6762), etc.
Throw a [Javascript API](https://www.w3.org/TR/webtransport/) on top of these and you have WebRTC.

<figure>
	![WebRTC protocols and layers](/blog/replacing-webrtc/layers.png)
	<figcaption>[Source](https://hpbn.co/webrtc/). That's a lot of protocols layered on top of each other.</figcaption>
</figure>

## Why not WebRTC?

I wouldn't be writing this blog post if WebRTC was perfect.
The core issue is that WebRTC is not a protocol; it's a monolith.

WebRTC does a lot of things, let's break down it down piece by piece:

-   [Media](#media): a full capture/encoding/networking/rendering pipeline.
-   [Data](#data): reliable/unreliable messages.
-   [P2P](#p2p): peer-to-peer connectability.
-   [SFU](#sfu): a relay that selectively forwards media.

### Media

The WebRTC media stack is designed for conferencing and does an amazing job at it.
The problems start when you try to use it for anything else.

My final project at Twitch was to reduce latency by replacing HLS with WebRTC for delivery.
This seems like a no-brainer at first, but it quickly turned into [death by a thousand cuts](https://docs.google.com/document/d/1OTnJunbpSJchdj8XI3GU9Fo-RUUFBqLO1AhlaKk5Alo/edit?usp=sharing).
The biggest issue was that the user experience was just terrible.
Twitch doesn't need the same aggressive latency as Google Meet, but WebRTC is hard-wired to compromise on quality.

In general, it's quite difficult to customize WebRTC outside of a few configurable modes.
It's a black box that you turn on, and if it works it works.
And if it doesn't work, then you have to deal with the pain that is [forking libwebrtc](https://github.com/webrtc-sdk/libwebrtc)... or just hope Google fixes it for you.

The protocol has some wiggle room and I really enjoyed my time tinkering with [pion](https://github.com/pion/webrtc).
But you're ultimately bound by the browser implementation, unless you don't need web support, in which case you don't need WebRTC.

### Data

WebRTC also has a data channel API, which is particularly useful because [until recently](https://developer.mozilla.org/en-US/docs/Web/API/WebTransport), it's been the only way to send/receive unreliable messages from a browser.

In fact, many companies use WebRTC data channels to avoid the WebRTC media stack (ex. Zoom).
I went down this path too, attempting to send each video frame as an unreliable message, but ultimately it didn't work due to fundamental flaws with [SCTP](https://www.rfc-editor.org/rfc/rfc4960.html).

I eventually hacked "datagram" support into SCTP by breaking frames into unreliable messages below the MTU size.
Finally! UDP in the browser, but at what cost:

-   a convoluted handshake that takes at least 10 (!) round trips.
-   2x the packets, because libsctp immediately ACKs every "datagram".
-   a custom SCTP implementation, which means the browser can't send "datagrams".

Oof. Fortunately there's now a [better way](https://developer.mozilla.org/en-US/docs/Web/API/WebTransport/datagrams) to send datagrams.
Feel free to use my [Rust library](https://docs.rs/webtransport-quinn/latest/webtransport_quinn/).

### P2P

The best and worst part about WebRTC is that it supports peer-to-peer.
The ICE handshake is extremely complicated:

-   You need both peers to generate a SDP offer/answer.
-   You need a public server (usually HTTPS/WebSocket) to exchange the offer/answer.
-   Some networks use symmetric NATs, so you need a fallback TURN server.
-   Some networks block UDP, so you need a fallback TCP TURN server.
-   Some networks only support IPv4 or IPv6 internally, so you should support dual-stack.
-   Some clients only support mDNS, so you have to figure that out too.

Most conferencing solutions these days are actually client-server for better QoS.
However, you're still forced to perform the complicated ICE handshake.
This has major architecture ramifications, but I'll save that for another blog post.

Note that there are rumblings of [P2P WebTransport](https://w3c.github.io/p2p-webtransport/) and [P2P QUIC](https://datatracker.ietf.org/doc/draft-seemann-quic-nat-traversal/), but I wouldn't hold my breath.

### SFU

Last but not least, WebRTC scales using SFUs (Selective Forwarding Units).

<figure>
	![SFU example](/blog/replacing-webrtc/sfu.png)
	<figcaption>
		[Source](https://blog.livekit.io/scaling-webrtc-with-distributed-mesh/). Participants send to a central server,
		rather than directly to each other.
	</figcaption>
</figure>

The problem with SFUs is subtle: they're custom.

It requires a lot of business logic to determine _where_ to forward packets.
A single server like that diagram won't scale, nor will all of the participants be located in the same geo.
Each SFU needs to be made aware of the network topology and the location of each participant _somehow_.

Additionally, a good SFU will avoid dropping packets based on dependencies, otherwise you waste bandwidth on undecodable packets.
Unfortunately, determining this requires parsing each RTP packet on a _per-codec_ basis.
For example, here's a [h.264 depacketizer](https://webrtc.googlesource.com/src/+/refs/heads/main/modules/rtp_rtcp/source/video_rtp_depacketizer_h264.cc) for libwebrtc.

But the biggest issue at Twitch was that SFUs share very little in common with CDNs.
One team is optimizing WebRTC, another team is optimizing HTTP, and they're not talking to each other.

This is why HLS/DASH uses HTTP instead: **economies of scale**

# Replacing WebRTC

Okay enough ranting about what's wrong, let's fix it.

First off, **WebRTC is not going anywhere**. It does a fantastic job at what it was designed for: conferencing.
It will take a long time before anything will reach feature/latency parity with WebRTC.

Before you can replace **Web**RTC, you need **Web**Support.
Fortunately, we now have **Web**Codecs and **Web**Transport.

## WebCodecs

[WebCodecs](https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API) is a new API for encoding/decoding media in the browser.
It's remarkably simple:

1. Capture input via [canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API) or a [media device](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia).
2. [VideoEncoder](https://developer.mozilla.org/en-US/docs/Web/API/VideoEncoder): Input raw frames, output encoded frames.
3. Transfer those frames somehow. (ex. [WebTransport](#webtransport))
4. [VideoDecoder](https://developer.mozilla.org/en-US/docs/Web/API/VideoDecoder): Input encoded frames, output raw frames.
5. Render output via [canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API) or just marvel at the pixel data.

The catch is that the application is responsible for all timing.
That means you need to choose when to render each frame via [requestAnimationFrame](https://developer.mozilla.org/en-US/docs/Web/API/window/requestAnimationFrame).
In fact, you need to choose when to render each audio _sample_ via [AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet).

The upside is that now your web application gets full control of how to render media.
It's now possible to implement WebRTC-like behavior, like temporarily freezing video and desyncing A/V.

[caniuse](https://caniuse.com/webcodecs)

## WebTransport

[WebTransport](https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API) is a new API for transmitting data over the network.
Think of it like WebSockets, but with a few key differences:

-   [QUIC](https://www.rfc-editor.org/rfc/rfc9000.html) not TCP.
-   Provides independent streams that can be closed/prioritized.
-   Provides datagrams that can be dropped.

QUIC has too many benefits to enumerate, but some highlights:

-   Fully encrypted
-   Congestion controlled (even datagrams)
-   1-RTT handshake
-   Multiplexed over a single UDP port
-   Transparent network migration (ex. switching from Wifi to LTE)
-   Used for HTTP/3

That last one is surprisingly important: WebTransport will share all of the optimizations that HTTP/3 receives.
A HTTP/3 server can simultaneously serve multiple WebTransport sessions and HTTP requests over the same connection.

[caniuse](https://caniuse.com/webtransport)

## But how?

Okay, so we have WebCodecs and WebTransport, but are they actually useful?

I alluded to the secret behind latency earlier: avoiding queues.
Queuing can occur at any point in the media pipeline.

| Capture | Encode | Send | Receive | Decode | Render |
| :-----: | :----: | :--: | :-----: | :----: | :----: |
|   -->   |  -->   | -->  |   -->   |  -->   |  -->   |

Let's start with the easy one.
[WebCodecs](#webcodecs) allows you to avoid queuing almost entirely.

|    Capture    |    Encode     | Send | Receive |    Decode     |    Render     |
| :-----------: | :-----------: | :--: | :-----: | :-----------: | :-----------: |
| **WebCodecs** | **WebCodecs** |  ?   |    ?    | **WebCodecs** | **WebCodecs** |

The tricky part is the bit in the middle, the network.

### The Internet of Queues

The internet is a [series of tubes](https://en.wikipedia.org/wiki/Series_of_tubes).
You put packets in one end and they eventually come out of the other end, kinda.
This section will get an entire blog post in the future, but until then, let's over-simplify things.

Every packet you send will fight with other packets on the internet.

-   If routers have sufficient throughput, **packets arrive on time**.
-   If routers have limited throughput, **packets will be queued**.
-   If those queues are full, **packets will be dropped**.

There can be random packet loss, but 99% of the time we care about loss due to queuing.

### Detecting Queuing

The goal of congestion control is to detect queuing and back off.

Different congestion control algorithms use different signals to detect queuing.
This is a gross oversimplification of a topic with an immense amount of research, but here's a rough breakdown:

|    Signal    | Description                                           |                      Latency                      |                                                          Examples                                                          |
| :----------: | :---------------------------------------------------- | :-----------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------: |
| Packet Loss  | Wait until the queue is full and packets are dropped. | [High](https://en.wikipedia.org/wiki/Bufferbloat) |       [Reno](https://en.wikipedia.org/wiki/TCP_congestion_control), [CUBIC](https://en.wikipedia.org/wiki/CUBIC_TCP)       |
|  ACK Delay   | Indirectly measure the queue size via ACK RTT.        |                      Medium                       |                      [BBR](https://research.google/pubs/pub45646/), [COPA](https://web.mit.edu/copa/)                      |
| Packet Delay | Indirectly measure the queue size via packet RTT.     |                        Low                        | [GCC](https://datatracker.ietf.org/doc/html/draft-ietf-rmcat-gcc-02), [SCReAM](https://github.com/EricssonResearch/scream) |
|     ECN      | Get told by the router to back off.                   |                      None\*                       |                                      [L4S](https://datatracker.ietf.org/doc/rfc9330/)                                      |

There's no single "best" congestion control algorithm; it depends on your use-case, network, and target latency.
But this is one area where WebRTC has a huge advantage thanks to [transport-wide-cc](https://webrtc.googlesource.com/src/+/refs/heads/main/docs/native-code/rtp-hdrext/transport-wide-cc-02/README.md).

### Reducing Bitrate

Once you detect queuing, the application needs to send fewer bytes.

In some situations we can just reduce the encoder bitrate, however:

-   This only applies to future frames.
-   We don't want one viewer to degrade the experience for all.
-   It's too expensive to encode on a per-viewer basis.

So basically, we have to drop encoded media in response to congestion.

This is the fundamental problem with TCP.
Once you queue data on a TCP socket, it can't be undone without closing the connection.
You can't put the toothpaste back in the tube.

<figure>
	![TCP toothpaste](/blog/replacing-webrtc/toothpaste.jpg)
	<figcaption>
		[Source](https://knowyourmeme.com/memes/shitting-toothpaste-pooping-toothpaste). You earned a meme for making it
		this far.
	</figcaption>
</figure>

However, there are actually quite a few ways of dropping media with [WebTransport](#webtransport):

1. Use datagrams and choose which packets to transmit. (like WebRTC)
2. Use QUIC streams and reset them to stop transmitting. (like [RUSH](https://www.ietf.org/archive/id/draft-kpugin-rush-00.html))
3. Use QUIC streams and prioritize them. (like [Warp](https://www.youtube.com/watch?v=PncdrMPVaNc))

I'm biased because I made the 3rd one.
WebTransport's [sendOrder](https://www.w3.org/TR/webtransport/#dom-webtransportsendstreamoptions-sendorder) can be used to instruct the QUIC stack what should be sent during congestion.
But that deserves an entire blog post on its own.

# Replacing WebRTC

But to actually replace WebRTC, we need a standard. Anybody can make their own UDP-based protocol (_and they do_), using this new web tech (_and they will_).

What sets [Media over QUIC](https://datatracker.ietf.org/wg/moq/about/) apart is that we're doing it through the IETF, the same organization that standardized WebRTC... and virtually every internet protocol.

It's going to take years.<br />
It's going to take a lot of idiots like myself who want to replace WebRTC. <br />
It's going to take a lot of companies who are willing to bet on a new standard.<br />

And there are major flaws with both **WebCodecs** and **WebTransport** that still need to be addressed before we'll ever reach WebRTC parity.
To name a few:

-   We need something like [transport-wide-cc](https://webrtc.googlesource.com/src/+/refs/heads/main/docs/native-code/rtp-hdrext/transport-wide-cc-02/README.md) in QUIC.
-   We need better [congestion control](https://www.w3.org/TR/webtransport/#dom-webtransportoptions-congestioncontrol) in browsers.
-   We need echo cancellation in WebCodecs, which might be possible already?
-   We need [FEC](https://datatracker.ietf.org/doc/draft-michel-quic-fec/) in QUIC, at least to experiment.
-   We need more encoding options, like non-reference frames or SVC.
-   Oh yeah and full browser support: [WebCodecs](https://caniuse.com/webcodecs) - [WebTransport](https://caniuse.com/webtransport)

## So yeah...

Hit us up on [Discord](https://discord.gg/FCYF3p99mr) if you want to help!
